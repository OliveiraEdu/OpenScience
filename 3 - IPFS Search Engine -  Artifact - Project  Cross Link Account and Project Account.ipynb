{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759b5251-2d99-444d-890b-0272c036750d",
   "metadata": {},
   "source": [
    "**IMPORTANT** \n",
    "\n",
    "- For requirements and initial setup go to https://github.com/OliveiraEdu/OpenScience/Readme.md;\n",
    "- To execute the notebook run all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8923b08-9526-4300-a2ad-3c943be049ba",
   "metadata": {},
   "source": [
    " # Cross Linking Account and Project accounts, IPFS Search Engine implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11965914-6a6a-43b4-a793-01a3eda28617",
   "metadata": {},
   "source": [
    "# Part - 1 Cross Linking Account and Project accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab571c6-459e-4d8c-a14f-8ea40c12fdbe",
   "metadata": {},
   "source": [
    "## Activities\n",
    "\n",
    "1 - Deploys a smart contract into the Iroha 1 blockchain for details (attributes) setting;\n",
    "\n",
    "2 - User and Project id extraction from JSON-LD files;\n",
    "\n",
    "3 - Queries Iroha 1 for User and Project accounts and checks the present values;\n",
    "\n",
    "4 - Sets details for both User and Project accounts in Iroha 1 providing a logical link between them for later references;\n",
    "\n",
    "5 - Queries the User and Project accounts again and checks the proper setting of details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79226aee-cfb3-4a03-b85a-18468e43a9aa",
   "metadata": {},
   "source": [
    "## Sequence Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aad64b-9986-49bf-bf43-1039243c4dd3",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "\n",
    "%%{\n",
    "  init: {\n",
    "    'theme': 'base',\n",
    "    'themeVariables': {\n",
    "      'primaryColor': '#ffffff',\n",
    "      'primaryTextColor': '#000000',\n",
    "      'primaryBorderColor': '#000000',\n",
    "      'lineColor': '#000000',\n",
    "      'secondaryColor': '#f4f4f4',\n",
    "      'secondaryTextColor': '#000000',\n",
    "      'tertiaryColor': '#d3d3d3',\n",
    "      'tertiaryTextColor': '#000000',\n",
    "      'background': '#ffffff',\n",
    "      'actorBkg': '#B4B4B4',\n",
    "      'actorTextColor': '#000000',\n",
    "      'actorBorder': '#000000',\n",
    "      'actorLineColor': '#000000',\n",
    "      'signalColor': '#000000',\n",
    "      'signalTextColor': '#000000',\n",
    "      'activationBorderColor': '#000000',\n",
    "      'activationBkgColor': '#d3d3d3',\n",
    "      'sequenceNumberColor': '#000000',\n",
    "      'noteBkgColor': '#F0F0F0',\n",
    "      'noteTextColor': '#000000',\n",
    "      'noteBorderColor': '#000000'\n",
    "    }\n",
    "  }\n",
    "}%%\n",
    "\n",
    "sequenceDiagram\n",
    "    participant Platform as \"Platform\"\n",
    "    participant self as \"self\"\n",
    "    participant Blockchain as \"Iroha 1 Blockchain\"\n",
    "    \n",
    "\n",
    "    Note over Platform, Blockchain: Deploy smart contract for details setting\n",
    "    Platform->>Blockchain: Deploy Smart Contract\n",
    "    Blockchain-->>Platform: Smart Contract Deployed Successfully\n",
    "\n",
    "    Note over Platform, Blockchain: Extract user and project IDs from JSONs\n",
    "    Platform->>self: User ID Extraction\n",
    "    Platform->>self: Project ID Extraction\n",
    "\n",
    "    Note over Platform, Blockchain: Queries the blockchain for <br/>User and Project accounts details\n",
    "    Platform->>Blockchain: Get User Account Details\n",
    "    Blockchain-->>Platform: Query Response\n",
    "    Platform->>Blockchain: Get Project Account Details\n",
    "    Blockchain-->>Platform: Query Response\n",
    "    \n",
    "    Note over Platform, Blockchain: Set details for User and Project accounts\n",
    "    Platform->>Blockchain: Set User Details in Blockchain\n",
    "    Blockchain-->>Platform: User Details Set Successfully\n",
    "    Platform->>Blockchain: Set Project Details in Blockchain\n",
    "    Blockchain-->>Platform: Project Details Set Successfully\n",
    "    \n",
    "    Note over Platform, Blockchain: Queries the blockchain to <br/>confirm proper setting of details\n",
    "    Platform->>Blockchain: Get User Account Details\n",
    "    Blockchain-->>Platform: Query Response\n",
    "    Platform->>Blockchain: Get Project Account Details\n",
    "    Blockchain-->>Platform: Query Response\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a043d51-4a22-4186-975e-7e3e8a4b54ef",
   "metadata": {},
   "source": [
    "1 - Deploys a smart contract into the Iroha 1 blockchain for details (attributes) setting;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4a1c5a2-75c3-482e-999c-70eb25226078",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEntering \"create_contract\"\n",
      "('STATELESS_VALIDATION_SUCCESS', 1, 0)\n",
      "('ENOUGH_SIGNATURES_COLLECTED', 9, 0)\n",
      "('STATEFUL_VALIDATION_SUCCESS', 3, 0)\n",
      "('COMMITTED', 5, 0)\n",
      "\tLeaving \"create_contract\"\n",
      "\tEntering \"get_engine_receipts_result\"\n",
      "\n",
      "\tLeaving \"get_engine_receipts_result\"\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from Crypto.Hash import keccak\n",
    "import os\n",
    "import binascii\n",
    "from iroha import IrohaCrypto\n",
    "from iroha import Iroha, IrohaGrpc\n",
    "from iroha.ed25519 import H\n",
    "import integration_helpers\n",
    "from iroha.primitive_pb2 import can_set_my_account_detail\n",
    "import sys\n",
    "# import csv\n",
    "import json\n",
    "import icecream as ic\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 or a more recent version is required.\")\n",
    "\n",
    "# Load configuration from config.json file\n",
    "config_path = \"config.json\"  # Update this path as needed\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "IROHA_HOST_ADDR = config[\"IROHA_HOST_ADDR\"]\n",
    "IROHA_PORT = config[\"IROHA_PORT\"]\n",
    "ADMIN_ACCOUNT_ID = config[\"ADMIN_ACCOUNT_ID\"]\n",
    "ADMIN_PRIVATE_KEY = config[\"ADMIN_PRIVATE_KEY\"]\n",
    "\n",
    "iroha = Iroha(ADMIN_ACCOUNT_ID)\n",
    "net = IrohaGrpc(\"{}:{}\".format(IROHA_HOST_ADDR, IROHA_PORT))\n",
    "\n",
    "\n",
    "@integration_helpers.trace\n",
    "def create_contract():\n",
    "    bytecode = \"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\"\n",
    "    \"\"\"Bytecode was generated using remix editor  https://remix.ethereum.org/ from file detail.sol. \"\"\"\n",
    "    tx = iroha.transaction(\n",
    "        [iroha.command(\"CallEngine\", caller=ADMIN_ACCOUNT_ID, input=bytecode)]\n",
    "    )\n",
    "    IrohaCrypto.sign_transaction(tx, ADMIN_PRIVATE_KEY)\n",
    "    net.send_tx(tx)\n",
    "    hex_hash = binascii.hexlify(IrohaCrypto.hash(tx))\n",
    "    for status in net.tx_status_stream(tx):\n",
    "        print(status)\n",
    "    return hex_hash\n",
    "\n",
    "hash = create_contract()\n",
    "integration_helpers.get_engine_receipts_result(hash)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea1051-cb1c-4c29-97e5-38ba90248081",
   "metadata": {},
   "source": [
    "2 - Data extraction from JSON-LD.\n",
    "\n",
    "Extracts account ids from `datasets/accounts.json` and `datasets/projects.json`.\n",
    "\n",
    "Must update `json_ld_index` with a entry number related to an existing object in `datasets/accounts.json` and `datasets/projects.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f8384a9-49db-44a2-b222-f3657a3bc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index for objects in both user account and project account JSON-LDs.\n",
    "json_ld_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e068d0-2247-4fd3-827f-4144b66965e1",
   "metadata": {},
   "source": [
    "4 - Sets details for both User and Project accounts providing a logical link between them for later references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "126df4cd-d358-43cc-997f-e17453a5b8dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEntering \"get_engine_receipts_address\"\n",
      "\tLeaving \"get_engine_receipts_address\"\n",
      "\tEntering \"get_engine_receipts_address\"\n",
      "\tLeaving \"get_engine_receipts_address\"\n",
      "('STATELESS_VALIDATION_SUCCESS', 1, 0)\n",
      "('ENOUGH_SIGNATURES_COLLECTED', 9, 0)\n",
      "('STATEFUL_VALIDATION_SUCCESS', 3, 0)\n",
      "('COMMITTED', 5, 0)\n",
      "('STATELESS_VALIDATION_SUCCESS', 1, 0)\n",
      "('ENOUGH_SIGNATURES_COLLECTED', 9, 0)\n",
      "('STATEFUL_VALIDATION_SUCCESS', 3, 0)\n",
      "('COMMITTED', 5, 0)\n",
      "Updated user account affectionate_northcutt@test with linked project 74933@test\n",
      "Updated project account 74933@test with linked user affectionate_northcutt@test\n",
      "User account affectionate_northcutt@test linked to project 74933@test\n",
      "Project account 74933@test linked to user affectionate_northcutt@test\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import binascii\n",
    "\n",
    "address = integration_helpers.get_engine_receipts_address(hash)\n",
    "\n",
    "\n",
    "# Function to link details using blockchain\n",
    "def set_account_detail(address, account, key, value):\n",
    "    params = integration_helpers.get_first_four_bytes_of_keccak(\n",
    "        b\"setAccountDetail(string,string,string)\"\n",
    "    )\n",
    "    no_of_param = 3\n",
    "    for x in range(no_of_param):\n",
    "        params = params + integration_helpers.left_padded_address_of_param(\n",
    "            x, no_of_param\n",
    "        )\n",
    "    params = params + integration_helpers.argument_encoding(account)  # account id\n",
    "    params = params + integration_helpers.argument_encoding(key)  # key\n",
    "    params = params + integration_helpers.argument_encoding(value)  # value\n",
    "    tx = iroha.transaction(\n",
    "        [\n",
    "            iroha.command(\n",
    "                \"CallEngine\", caller=ADMIN_ACCOUNT_ID, callee=address, input=params\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    IrohaCrypto.sign_transaction(tx, ADMIN_PRIVATE_KEY)\n",
    "    response = net.send_tx(tx)\n",
    "    for status in net.tx_status_stream(tx):\n",
    "        print(status)\n",
    "    hex_hash = binascii.hexlify(IrohaCrypto.hash(tx))\n",
    "    return hex_hash\n",
    "\n",
    "# Function to update user account with linked project\n",
    "def update_user_account_link(user_account_id, linked_project_id, accounts_filename=\"datasets/accounts.json\"):\n",
    "    try:\n",
    "        if os.path.exists(accounts_filename):\n",
    "            with open(accounts_filename, mode='r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "        else:\n",
    "            print(f\"{accounts_filename} does not exist.\")\n",
    "            return\n",
    "\n",
    "        user_found = False\n",
    "\n",
    "        # Look for the user account and update it\n",
    "        for entry in data[\"@graph\"]:\n",
    "            if entry[\"@type\"] == \"foaf:Person\" and entry.get(\"foaf:holdsAccount\", {}).get(\"schema:identifier\") == user_account_id:\n",
    "                entry[\"schema:linked_project\"] = linked_project_id\n",
    "                user_found = True\n",
    "                print(f\"Updated user account {user_account_id} with linked project {linked_project_id}\")\n",
    "                break\n",
    "\n",
    "        if not user_found:\n",
    "            print(f\"User account {user_account_id} not found.\")\n",
    "\n",
    "        # Write back the updated data\n",
    "        with open(accounts_filename, mode='w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating user account in JSON-LD: {str(e)}\")\n",
    "\n",
    "\n",
    "# Function to update project account with linked user\n",
    "def update_project_account_link(project_account_id, linked_user_id, projects_filename=\"datasets/projects.json\"):\n",
    "    try:\n",
    "        if os.path.exists(projects_filename):\n",
    "            with open(projects_filename, mode='r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "        else:\n",
    "            print(f\"{projects_filename} does not exist.\")\n",
    "            return\n",
    "\n",
    "        project_found = False\n",
    "\n",
    "        # Look for the project account and update it\n",
    "        for entry in data[\"@graph\"]:\n",
    "            if entry[\"@type\"] == \"schema:ResearchProject\" and entry.get(\"schema:identifier\") == project_account_id:\n",
    "                entry[\"schema:linked_user\"] = linked_user_id\n",
    "                project_found = True\n",
    "                print(f\"Updated project account {project_account_id} with linked user {linked_user_id}\")\n",
    "                break\n",
    "\n",
    "        if not project_found:\n",
    "            print(f\"Project account {project_account_id} not found.\")\n",
    "\n",
    "        # Write back the updated data\n",
    "        with open(projects_filename, mode='w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating project account in JSON-LD: {str(e)}\")\n",
    "\n",
    "\n",
    "# Function to read accounts from JSON-LD\n",
    "def read_user_accounts_from_jsonld(file_path):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        user_accounts = []\n",
    "        for entry in data[\"@graph\"]:\n",
    "            if entry[\"@type\"] == \"foaf:Person\":\n",
    "                account_id = entry.get(\"foaf:holdsAccount\", {}).get(\"schema:identifier\")\n",
    "                if account_id:\n",
    "                    user_accounts.append({'account_id': account_id})\n",
    "        return user_accounts\n",
    "\n",
    "\n",
    "def read_project_accounts_from_jsonld(file_path):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        project_accounts = []\n",
    "        for entry in data[\"@graph\"]:\n",
    "            if entry[\"@type\"] == \"schema:ResearchProject\":\n",
    "                project_id = entry.get(\"schema:identifier\")\n",
    "                if project_id:\n",
    "                    project_accounts.append({'account_id': project_id})\n",
    "        return project_accounts\n",
    "\n",
    "\n",
    "# Example execution of the previous snippet\n",
    "address = integration_helpers.get_engine_receipts_address(hash)\n",
    "\n",
    "# Read accounts from JSON-LD\n",
    "user_accounts = read_user_accounts_from_jsonld('datasets/accounts.json')\n",
    "project_accounts = read_project_accounts_from_jsonld('datasets/projects.json')\n",
    "\n",
    "# Assuming json_ld_index is defined\n",
    "user_account = user_accounts[json_ld_index]\n",
    "project_account = project_accounts[json_ld_index]\n",
    "\n",
    "# Set project_id as a detail for the user account\n",
    "hash_user_to_project = set_account_detail(\n",
    "    address, \n",
    "    user_account['account_id'], \n",
    "    \"linked_project\", \n",
    "    project_account['account_id']\n",
    ")\n",
    "\n",
    "# Set user_account_id as a detail for the project account\n",
    "hash_project_to_user = set_account_detail(\n",
    "    address, \n",
    "    project_account['account_id'], \n",
    "    \"linked_user\", \n",
    "    user_account['account_id']\n",
    ")\n",
    "\n",
    "# Update the JSON-LD files with the linked details\n",
    "update_user_account_link(user_account['account_id'], project_account['account_id'])\n",
    "update_project_account_link(project_account['account_id'], user_account['account_id'])\n",
    "\n",
    "# Confirming the operation\n",
    "print(f\"User account {user_account['account_id']} linked to project {project_account['account_id']}\")\n",
    "print(f\"Project account {project_account['account_id']} linked to user {user_account['account_id']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562857c-d86d-42cf-a24b-23a5c3f5cb4e",
   "metadata": {},
   "source": [
    "3 - Queries Iroha 1 for User account and checks its values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b52e47a4-b1c5-4ebe-a2d8-8edae10063d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Account id = {'account_id': 'affectionate_northcutt@test'}, { \"admin@test\" : { \"linked_project\" : \"74933@test\", \"user_json_ld_cid\" : \"QmQao9pyRokdkYTc236hCwuZKdohDY2N3gsC5dcqwmAQnj\" } }\n"
     ]
    }
   ],
   "source": [
    "#Query - GetAccountDetail\n",
    "query = iroha.query('GetAccountDetail',account_id=user_account['account_id'])\n",
    "# print(query)\n",
    "IrohaCrypto.sign_query(query, ADMIN_PRIVATE_KEY)\n",
    "response = net.send_query(query)\n",
    "# print(response)\n",
    "\n",
    "user_data = response.account_detail_response\n",
    "user_details = user_data.detail\n",
    "\n",
    "print(f'User Account id = {user_account}, {user_details}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a556f-b657-4a6a-8c9f-b18aac8f8375",
   "metadata": {},
   "source": [
    "3 - Queries Iroha 1 for Project account and checks its values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d39cb27c-4f7d-48dd-a137-6bceb294c15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Account id = {'account_id': '74933@test'}, { \"admin@test\" : { \"linked_user\" : \"affectionate_northcutt@test\", \"project_metadata_cid\" : \"Qmf8ykuh62dkPoihRPk5bDh35F6uye21ocvzUwfusK9xEq\" } }\n"
     ]
    }
   ],
   "source": [
    "#Query - GetAccountDetail\n",
    "query = iroha.query('GetAccountDetail',account_id=project_account['account_id'])\n",
    "IrohaCrypto.sign_query(query, ADMIN_PRIVATE_KEY)\n",
    "response = net.send_query(query)\n",
    "# print(response)\n",
    "project_data = response.account_detail_response\n",
    "project_details = project_data.detail\n",
    "print(f'Project Account id = {project_account}, {project_details}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499f660b-56d6-4ea0-bd2b-e3bb54ecd05b",
   "metadata": {},
   "source": [
    "# Part2 - Querying Project Metadata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986de21a-9ddc-4aea-b63d-47a6a0323871",
   "metadata": {},
   "source": [
    "## Activities\n",
    "\n",
    "6 - Queries the user account, locates the project id, queries the project account, gets the metadata and files from IPFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0445f-998d-4470-9dfa-252caf810921",
   "metadata": {},
   "source": [
    "## Sequence Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e812f7b-8439-4e65-883c-de57b8e81097",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "%%{\n",
    "  init: {\n",
    "    'theme': 'base',\n",
    "    'themeVariables': {\n",
    "      'primaryColor': '#ffffff',\n",
    "      'primaryTextColor': '#000000',\n",
    "      'primaryBorderColor': '#000000',\n",
    "      'lineColor': '#000000',\n",
    "      'secondaryColor': '#f4f4f4',\n",
    "      'secondaryTextColor': '#000000',\n",
    "      'tertiaryColor': '#d3d3d3',\n",
    "      'tertiaryTextColor': '#000000',\n",
    "      'background': '#ffffff',\n",
    "      'actorBkg': '#B4B4B4',\n",
    "      'actorTextColor': '#000000',\n",
    "      'actorBorder': '#000000',\n",
    "      'actorLineColor': '#000000',\n",
    "      'signalColor': '#000000',\n",
    "      'signalTextColor': '#000000',\n",
    "      'activationBorderColor': '#000000',\n",
    "      'activationBkgColor': '#d3d3d3',\n",
    "      'sequenceNumberColor': '#000000',\n",
    "      'noteBkgColor': '#F0F0F0',\n",
    "      'noteTextColor': '#000000',\n",
    "      'noteBorderColor': '#000000'\n",
    "    }\n",
    "  }\n",
    "}%%\n",
    "\n",
    "sequenceDiagram\n",
    "    participant Platform as \"Platform\"\n",
    "    participant Blockchain as \"Iroha 1 Blockchain\"\n",
    "    participant IPFS as \"Interplanetary File System\"\n",
    "    participant FrontEnd as \"Front End\"\n",
    "\n",
    "    Note over Platform, Blockchain: Queries the user account <br/> and get the project id \n",
    "    Platform->>Blockchain: Query User Account Details\n",
    "    Blockchain-->>Platform: Query Response\n",
    "        \n",
    "    Note over Platform, Blockchain: Queries the Project Account details <br/> and get the project metadata CID \n",
    "    Platform->>Blockchain: Query Project Account Details\n",
    "    Blockchain-->>Platform: Query Response\n",
    "\n",
    "    Note over Platform, IPFS: Process and displays the metadata CID \n",
    "    Platform->>IPFS: Sends the project metadata CID\n",
    "    IPFS-->>Platform: Sends back the project metadata JSON\n",
    "    Platform->>FrontEnd: Displays the project metadata JSON   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f33098-43df-47f3-8708-f3212dec1c0f",
   "metadata": {},
   "source": [
    "6 - Queries the user account, locates the project id, queries the project account, gets the metadata and files from IPFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "374843b3-6862-4a93-b298-1bdd0320c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'admin@test': {'linked_project': '74933@test', 'user_json_ld_cid': 'QmQao9pyRokdkYTc236hCwuZKdohDY2N3gsC5dcqwmAQnj'}}\n",
      "74933@test\n"
     ]
    }
   ],
   "source": [
    "from ipfs_functions import *\n",
    "\n",
    "# Process the account details response\n",
    "user_details_dict = json.loads(user_details)  # Convert the string to a JSON object\n",
    "print(user_details_dict)\n",
    "\n",
    "# Now you can access the specific key like this\n",
    "linked_project = user_details_dict[\"admin@test\"][\"linked_project\"]\n",
    "print(linked_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66b2f2c1-00b1-4b1b-8951-196af1f25208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Account id = {'account_id': '74933@test'}, { \"admin@test\" : { \"linked_user\" : \"affectionate_northcutt@test\", \"project_metadata_cid\" : \"Qmf8ykuh62dkPoihRPk5bDh35F6uye21ocvzUwfusK9xEq\" } }\n"
     ]
    }
   ],
   "source": [
    "#Query - GetAccountDetail\n",
    "query = iroha.query('GetAccountDetail', account_id=linked_project)\n",
    "IrohaCrypto.sign_query(query, ADMIN_PRIVATE_KEY)\n",
    "response = net.send_query(query)\n",
    "# print(response)\n",
    "project_data = response.account_detail_response\n",
    "project_details = project_data.detail\n",
    "print(f'Project Account id = {project_account}, {project_details}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e7130f8-ad7d-4d0b-a521-ba0de69e9271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qmf8ykuh62dkPoihRPk5bDh35F6uye21ocvzUwfusK9xEq\n",
      "{'@context': {'dc': 'http://purl.org/dc/terms/', 'schema': 'http://schema.org/'}, '@type': 'schema:ResearchProject', 'dc:abstract': 'This research focuses on the benefits and challenges posed by genetic algorithms for financial services, with an emphasis on its potential for disease prevention.', 'schema:endDate': '2025-04-05', 'schema:funding': {'@type': 'schema:Organization', 'schema:name': 'Department of Energy'}, 'schema:keywords': ['genetic algorithms', 'financial services', 'disease prevention'], 'schema:location': {'@type': 'schema:Place', 'schema:name': 'Lisbon, Portugal'}, 'schema:name': 'Assessing the Benefits of genetic algorithms for financial services', 'schema:startDate': '2021-09-26'}\n"
     ]
    }
   ],
   "source": [
    "# Convert the JSON string to a Python dictionary\n",
    "project_details_dict = json.loads(project_details)\n",
    "\n",
    "# Now you can access the specific key like this\n",
    "project_metadata_cid = project_details_dict[\"admin@test\"][\"project_metadata_cid\"]\n",
    "print(project_metadata_cid)\n",
    "\n",
    "\n",
    "project_metadata = download_json_from_ipfs(project_metadata_cid)\n",
    "\n",
    "# print(20*\"-\")\n",
    "\n",
    "print(project_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31163ee6-8c5a-4cd3-8951-9190665abd5a",
   "metadata": {},
   "source": [
    "# Part 3 - File Operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d265a-50e1-423d-9e53-26226afe7756",
   "metadata": {},
   "source": [
    "7 -  Sends every file in the `upload` directory to IPFS, extracts theirs respective metadata with Apache Tika and sends it to IPFS, get the CIDs back and store in Iroha as details of the project account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b419c69e-9fbb-474b-ab9b-4ffdc38403ee",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "\n",
    "%%{\n",
    "  init: {\n",
    "    'theme': 'base',\n",
    "    'themeVariables': {\n",
    "      'primaryColor': '#ffffff',\n",
    "      'primaryTextColor': '#000000',\n",
    "      'primaryBorderColor': '#000000',\n",
    "      'lineColor': '#000000',\n",
    "      'secondaryColor': '#f4f4f4',\n",
    "      'secondaryTextColor': '#000000',\n",
    "      'tertiaryColor': '#d3d3d3',\n",
    "      'tertiaryTextColor': '#000000',\n",
    "      'background': '#ffffff',\n",
    "      'actorBkg': '#B4B4B4',\n",
    "      'actorTextColor': '#000000',\n",
    "      'actorBorder': '#000000',\n",
    "      'actorLineColor': '#000000',\n",
    "      'signalColor': '#000000',\n",
    "      'signalTextColor': '#000000',\n",
    "      'activationBorderColor': '#000000',\n",
    "      'activationBkgColor': '#d3d3d3',\n",
    "      'sequenceNumberColor': '#000000',\n",
    "      'noteBkgColor': '#F0F0F0',\n",
    "      'noteTextColor': '#000000',\n",
    "      'noteBorderColor': '#000000'\n",
    "    }\n",
    "  }\n",
    "}%%\n",
    "\n",
    "sequenceDiagram\n",
    "    participant Platform as \"Platform\"\n",
    "    participant Metadata Extractor\n",
    "    participant Indexer\n",
    "    participant Blockchain as \"Iroha 1 Blockchain\"\n",
    "    participant IPFS as \"Interplanetary File System\"\n",
    "\n",
    "    Note over Platform, IPFS: File Operations \n",
    "    Platform->>IPFS: Upload local file to IPFS\n",
    "    IPFS-->>Platform: Send back file CIDs\n",
    "    Platform->>Blockchain: Set CID as Project Account Details\n",
    "    Blockchain-->>Platform:Details set successfully\n",
    "\n",
    "    Note over Platform, IPFS: File Metadata Operations\n",
    "    Platform->>Metadata Extractor: Parse file and extract metadata \n",
    "    Metadata Extractor->>Indexer: Send file metadata for indexing\n",
    "    Indexer->>IPFS: Store file metadata JSON\n",
    "    IPFS-->>Platform: Send back file metadata JSON CIDs\n",
    "    Platform->>Blockchain: Set file metadata JSON CID as Project Account Details\n",
    "    Blockchain-->>Platform:Details set successfully\n",
    "         \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4fcb7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Opened existing index.\n",
      "INFO:root:Index recreated.\n",
      "INFO:root:Processing file: COVID19-MLSF--A-multi-task-learning-based-stock-market_2023_Expert-Systems-w.pdf\n",
      "INFO:root:File COVID19-MLSF--A-multi-task-learning-based-stock-market_2023_Expert-Systems-w.pdf uploaded to IPFS with CID: QmUq29KRwpTdvScB5oYzEDobDHyb4N1f9eaZXm4VaMCgiW\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74933@test\n",
      "('STATELESS_VALIDATION_SUCCESS', 1, 0)\n",
      "('ENOUGH_SIGNATURES_COLLECTED', 9, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing file 'COVID19-MLSF--A-multi-task-learning-based-stock-market_2023_Expert-Systems-w.pdf': name 'linked_user' is not defined\n",
      "INFO:root:All documents processed and index committed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('STATEFUL_VALIDATION_SUCCESS', 3, 0)\n",
      "('COMMITTED', 5, 0)\n"
     ]
    }
   ],
   "source": [
    "### New version\n",
    "\n",
    "from whoosh.index import create_in, open_dir, EmptyIndexError  # Import EmptyIndexError\n",
    "from metadata_helper import *\n",
    "import os\n",
    "import logging\n",
    "import mimetypes\n",
    "from tika import parser  # Import Apache Tika\n",
    "\n",
    "print(project_account['account_id'])\n",
    "\n",
    "# Function to extract only Dublin Core related metadata\n",
    "def extract_dublin_core(metadata):\n",
    "    \"\"\"Extracts only Dublin Core related metadata.\"\"\"\n",
    "    return {k: v for k, v in metadata.items() if k.startswith('dc:') or k.startswith('dcterms:')}\n",
    "\n",
    "\n",
    "# Function to parse and index documents from a directory\n",
    "def parse_documents_in_directory(directory_path, schema, linked_project, recreate=True):\n",
    "    \"\"\"Parses documents in a directory and indexes them.\"\"\"\n",
    "    ix = recreate_index(schema) if recreate else create_in(\"indexdir\", schema)\n",
    "    writer = get_writer_with_retry(ix)\n",
    "    index = 1\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        logging.info(f\"Processing file: {filename}\")\n",
    "\n",
    "        if not os.path.basename(filename).startswith('.'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            try:\n",
    "                # Upload the file to IPFS and get its CID\n",
    "                file_cid = upload_file_to_ipfs(file_path)\n",
    "                logging.info(f\"File {filename} uploaded to IPFS with CID: {file_cid}\")\n",
    "\n",
    "                \n",
    "                # Update the project details in the Iroha 1 blockchain\n",
    "                hash = set_account_detail(address, project_account['account_id'], f\"file_{index}_CID\", file_cid)\n",
    "\n",
    "\n",
    "                # Parse the document using Apache Tika\n",
    "                parsed_document = parser.from_file(file_path)\n",
    "\n",
    "                if parsed_document:\n",
    "                    metadata = parsed_document.get('metadata', {})\n",
    "                    full_text = parsed_document.get(\"content\", \"\").strip() or \"No content extracted\"\n",
    "\n",
    "                    # Extract Dublin Core related metadata\n",
    "                    dublin_core_metadata = extract_dublin_core(metadata)\n",
    "                    \n",
    "                    # Normalize and upload JSON metadata to IPFS\n",
    "                    normalized_metadata = {\n",
    "                        'user_id': linked_user,\n",
    "                        'project_id': linked_project,\n",
    "                        'cid': upload_json_to_ipfs(metadata),  # Upload the full metadata\n",
    "                        'name': filename,\n",
    "                        'size': os.path.getsize(file_path),\n",
    "                        'filetype': mimetypes.guess_type(filename)[0] or \"unknown\",\n",
    "                        'title': normalize_metadata_value(metadata.get(\"dc:title\", f\"Document {index}\")),\n",
    "                        'creator': normalize_metadata_value(metadata.get(\"dc:creator\", \"Unknown\")),\n",
    "                        'language': normalize_metadata_value(metadata.get(\"dc:language\", \"en\")),\n",
    "                        'subject': normalize_metadata_value(metadata.get(\"dc:subject\", \"\")),\n",
    "                        'description': normalize_metadata_value(metadata.get(\"dc:description\", \"\")),\n",
    "                        'publisher': normalize_metadata_value(metadata.get(\"dc:publisher\", \"Unknown\")),\n",
    "                        'date': normalize_metadata_value(metadata.get(\"dc:date\", \"\")),\n",
    "                        'abstract': normalize_metadata_value(metadata.get(\"dc:abstract\", \"\")),\n",
    "                        'format': normalize_metadata_value(metadata.get(\"dc:format\", \"\")),\n",
    "                        'created': normalize_metadata_value(metadata.get(\"dcterms:created\", \"\")),\n",
    "                        'modified': normalize_metadata_value(metadata.get(\"dcterms:modified\", \"\"))\n",
    "                    }\n",
    "\n",
    "                    ic(normalized_metadata)\n",
    "\n",
    "                    logging.info(f\"Indexed {filename} with CID: {normalized_metadata['cid']}\")\n",
    "\n",
    "                    # Add document to the Whoosh index\n",
    "                    add_document(writer, normalized_metadata, full_text)\n",
    "\n",
    "                    # Print extracted Dublin Core metadata\n",
    "                    print(\"Dublin Core Metadata:\")\n",
    "                    print(json.dumps(dublin_core_metadata, indent=4))\n",
    "\n",
    "                    # Update project entry with file data and Dublin Core metadata\n",
    "                    update_project_entry_with_file_data(\n",
    "                        linked_project, file_cid, normalized_metadata['cid'], dublin_core_metadata\n",
    "                    )\n",
    "\n",
    "                   \n",
    "                    \n",
    "\n",
    "                    # Upload the metadata to IPFS and get its CID\n",
    "                    print(40*\"-\")\n",
    "                    metadata_cid = upload_json_to_ipfs(metadata)\n",
    "                    logging.info(f\"File {filename} uploaded to IPFS with CID: {metadata_cid}\")\n",
    "                                        \n",
    "                    #Sets the project account detail with the file metadata\n",
    "                    print(40*\"-\")\n",
    "\n",
    "                    print(project_account['account_id'])\n",
    "                    print(index)\n",
    "                    hash = set_account_detail(\n",
    "                        address, project_account['account_id'], f\"file_{index}_metadata_CID\", metadata_cid\n",
    "                        )\n",
    "\n",
    "                    print(40*\"-\")\n",
    "                    \n",
    "                    #-\n",
    "\n",
    "                else:\n",
    "                    logging.error(f\"Parsing failed for '{filename}'.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file '{filename}': {e}\")\n",
    "                continue\n",
    "\n",
    "        logging.info(\"-\" * 40)\n",
    "        index += 1\n",
    "\n",
    "    writer.commit()  # Commit changes once all files are processed\n",
    "    logging.info(\"All documents processed and index committed.\")\n",
    "\n",
    "\n",
    "# Function to set up the Whoosh index directory\n",
    "def setup_index(schema):\n",
    "    \"\"\"Sets up the Whoosh index directory and returns the index object.\"\"\"\n",
    "    index_dir = \"indexdir\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "        logging.info(\"Index directory created.\")\n",
    "        ix = create_in(index_dir, schema)\n",
    "    else:\n",
    "        try:\n",
    "            ix = open_dir(index_dir)\n",
    "            logging.info(\"Opened existing index.\")\n",
    "        except EmptyIndexError:\n",
    "            logging.warning(\"Index is empty. Creating a new index.\")\n",
    "            ix = create_in(index_dir, schema)\n",
    "    return ix\n",
    "\n",
    "\n",
    "# Define the schema (including Dublin Core fields)\n",
    "schema = Schema(\n",
    "    linked_user=TEXT(stored=True),\n",
    "    linked_project=TEXT(stored=True),\n",
    "    cid=ID(stored=True),\n",
    "    name=TEXT(stored=True),\n",
    "    size=NUMERIC(stored=True),\n",
    "    filetype=TEXT(stored=True),\n",
    "    title=TEXT(stored=True),\n",
    "    creator=TEXT(stored=True),\n",
    "    language=TEXT(stored=True),\n",
    "    subject=TEXT(stored=True),\n",
    "    description=TEXT(stored=True),\n",
    "    publisher=TEXT(stored=True),\n",
    "    date=TEXT(stored=True),\n",
    "    abstract=TEXT(stored=True),\n",
    "    format=TEXT(stored=True),\n",
    "    created=TEXT(stored=True),  # Store as string (e.g., ISO format 'YYYY-MM-DD')\n",
    "    modified=TEXT(stored=True),  # Store as string (e.g., ISO format 'YYYY-MM-DD')\n",
    "    full_text=TEXT(stored=False)\n",
    ")\n",
    "\n",
    "# Setup index directory\n",
    "ix = setup_index(schema)\n",
    "\n",
    "# Example document parsing and indexing execution\n",
    "directory_path = \"upload\"\n",
    "linked_project = project_account['account_id']  # Example placeholder, adjust as needed\n",
    "\n",
    "# Parse documents in the specified directory\n",
    "parse_documents_in_directory(directory_path, schema, linked_project, recreate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74689d5",
   "metadata": {},
   "source": [
    "8 -  Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79938646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:No results found for 'tomorrow'\n"
     ]
    }
   ],
   "source": [
    "# Example search usage\n",
    "keyword = \"tomorrow\"\n",
    "search_index(keyword, ix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e6b78-9d2c-4543-b88c-ba216b3ed596",
   "metadata": {},
   "source": [
    "9 - Query the project account to verify the details update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66cfac8c-5f92-487a-87fd-067df3e26299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Account id = {'account_id': '74933@test'}, { \"admin@test\" : { \"file_1_CID\" : \"QmUq29KRwpTdvScB5oYzEDobDHyb4N1f9eaZXm4VaMCgiW\", \"linked_user\" : \"affectionate_northcutt@test\", \"project_metadata_cid\" : \"Qmf8ykuh62dkPoihRPk5bDh35F6uye21ocvzUwfusK9xEq\" } }\n"
     ]
    }
   ],
   "source": [
    "#Query - GetAccountDetail\n",
    "query = iroha.query('GetAccountDetail', account_id=linked_project)\n",
    "IrohaCrypto.sign_query(query, ADMIN_PRIVATE_KEY)\n",
    "response = net.send_query(query)\n",
    "# print(response)\n",
    "project_data = response.account_detail_response\n",
    "project_details = project_data.detail\n",
    "print(f'Project Account id = {project_account}, {project_details}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6ee05-f89d-44ce-aab8-e57b2acff3a4",
   "metadata": {},
   "source": [
    "10 - Read CIDs from Iroha and download file metadata and files from IPFS to the project home directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbac27b-655e-47e9-859a-b88629f5af14",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "%%{\n",
    "  init: {\n",
    "    'theme': 'base',\n",
    "    'themeVariables': {\n",
    "      'primaryColor': '#ffffff',\n",
    "      'primaryTextColor': '#000000',\n",
    "      'primaryBorderColor': '#000000',\n",
    "      'lineColor': '#000000',\n",
    "      'secondaryColor': '#f4f4f4',\n",
    "      'secondaryTextColor': '#000000',\n",
    "      'tertiaryColor': '#d3d3d3',\n",
    "      'tertiaryTextColor': '#000000',\n",
    "      'background': '#ffffff',\n",
    "      'actorBkg': '#B4B4B4',\n",
    "      'actorTextColor': '#000000',\n",
    "      'actorBorder': '#000000',\n",
    "      'actorLineColor': '#000000',\n",
    "      'signalColor': '#000000',\n",
    "      'signalTextColor': '#000000',\n",
    "      'activationBorderColor': '#000000',\n",
    "      'activationBkgColor': '#d3d3d3',\n",
    "      'sequenceNumberColor': '#000000',\n",
    "      'noteBkgColor': '#F0F0F0',\n",
    "      'noteTextColor': '#000000',\n",
    "      'noteBorderColor': '#000000'\n",
    "    }\n",
    "  }\n",
    "}%%\n",
    "\n",
    "sequenceDiagram\n",
    "    participant Platform as \"Platform\"\n",
    "    participant self as \"self\"\n",
    "    participant Blockchain as \"Iroha 1 Blockchain\"\n",
    "    participant IPFS as \"Interplanetary File System\"\n",
    "    participant FrontEnd as \"Front End\"\n",
    "    \n",
    "       \n",
    "    Note over Platform, Blockchain: Queries the Project Account details and get details\n",
    "    Platform->>Blockchain: Query Project Account Details\n",
    "    Blockchain->>Platform: Query Response\n",
    "\n",
    "    Note over Platform, IPFS: Process project account metadata\n",
    "    Platform->>self: Parse Project Details JSON and retrieve file CIDs\n",
    "\n",
    "    Note over Platform, IPFS: Download file from IPFS \n",
    "    Platform->>IPFS: Sends the file CID\n",
    "    IPFS->>Platform: Sends back the file\n",
    "    Platform->>FrontEnd:    Saves the file locally and display info and status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6ca74-7343-4109-b774-0582cb9bab9b",
   "metadata": {},
   "source": [
    "11 - Read details from the project account retrieve the CID of every file, download the it file from IPFS and store locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ae73161-a508-408c-81fb-3bb08b13fcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing project account: 74933@test\n",
      "Processing file CID: QmUq29KRwpTdvScB5oYzEDobDHyb4N1f9eaZXm4VaMCgiW for key: file_1_CID\n",
      "Skipping key: linked_user\n",
      "Skipping key: project_metadata_cid\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from clean_file_name import clean_file_name as clean_name\n",
    "from ipfs_functions import download_file_from_ipfs, download_json_from_ipfs\n",
    "\n",
    "# Convert the JSON string to a Python dictionary\n",
    "project_details_dict = json.loads(project_details)\n",
    "project_account_id = project_account['account_id']\n",
    "\n",
    "print(f\"Processing project account: {project_account_id}\")\n",
    "\n",
    "# Get the value of the dictionary (actual file metadata)\n",
    "files_metadata = project_details_dict['admin@test']\n",
    "# print(files_metadata)\n",
    "\n",
    "# Iterate through files metadata, skip unneeded keys, and process files and metadata CIDs\n",
    "for key, value in files_metadata.items():\n",
    "    if key in ['linked_user', 'project_metadata_cid']:\n",
    "        print(f\"Skipping key: {key}\")\n",
    "        continue\n",
    "\n",
    "    # Distinguish between file CID and metadata CID\n",
    "    if 'metadata_CID' not in key:\n",
    "        file_CID = value\n",
    "        print(f\"Processing file CID: {file_CID} for key: {key}\")\n",
    "    else:\n",
    "        file_metadata_key = '_'.join(key.split('_')[:-2])\n",
    "        file_metadata_CID = value\n",
    "        print(f\"Processing metadata CID: {file_metadata_CID} for key: {file_metadata_key}\")\n",
    "\n",
    "        # Download and process metadata JSON\n",
    "        file_metadata_json = download_json_from_ipfs(file_metadata_CID)\n",
    "        # print(file_metadata_json)\n",
    "\n",
    "        # Ensure 'resourceName' exists in metadata and process the file download\n",
    "        if 'resourceName' in file_metadata_json:\n",
    "            raw_file_name = file_metadata_json['resourceName']\n",
    "            cleaned_file_name = clean_name(raw_file_name)  # Renamed to avoid conflict\n",
    "            print(f\"Cleaned file name: {cleaned_file_name}\")\n",
    "\n",
    "            # Create user-specific download directory if it doesn't exist\n",
    "            download_directory = os.path.join(\"download\", project_account_id)\n",
    "            os.makedirs(download_directory, exist_ok=True)\n",
    "            print(f\"Download directory ready: {download_directory}\")\n",
    "\n",
    "            # Download file using the file CID\n",
    "            file_path = os.path.join(download_directory, cleaned_file_name)\n",
    "            print(f\"Downloading file to: {file_path}\")\n",
    "            download_file_from_ipfs(file_CID, file_path)\n",
    "            print(\"-\" * 40)\n",
    "        else:\n",
    "            print(f\"No 'resourceName' found for metadata CID: {file_metadata_CID}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
