\documentclass{article}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{array}
\usepackage{graphicx}  % For inserting images
\usepackage{caption}   % For better caption formatting
\usepackage{float}     % To control figure placement
\usepackage{natbib}  % Recommended for author-year citations
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{todonotes}


\title{Literature Review}
\author{Eduardo Oliveira}
\date{\today}

\begin{document}

\maketitle

\listoftodos


\section{Enhancing Reproducibility in Scientific Research Through Open Science and Decentralized Technologies}

\subsection{The Imperative of Reproducibility in Scientific Research Science}

Science as a systematic and empirical pursuit of knowledge, fundamentally relies on the ability of researchers to verify and build upon the findings of their predecessors and peers. At the core of this process lies the concept of reproducibility, which encompasses both the capacity for others to obtain consistent results using the same data and methods, and the ability to achieve similar findings when new data is collected through the same experimental design \cite{pellizzari_reproducibility_2017, committee_2019}. A significant concern has emerged within the scientific community regarding the difficulty of reproducing the results of numerous published scientific studies across a wide spectrum of disciplines. This phenomenon, frequently referred to as the "reproducibility crisis", has shaken the foundations of scientific inquiry, leading to a growing lack of trust in research findings \cite{baker2016reproducibility}. The concerningly high rates of non-reproducible research, with studies suggesting an average failure rate of 50\%, indicate a systemic issue that extends beyond isolated cases of flawed methodology or misconduct \cite{branch_reproducibility_2019}. To provide context on the financial impact of low reproducibility rates in the life sciences, estimated annual losses in the United States alone exceed \$28 billion, primarily attributed to research that fails to meet reproducibility standards \cite{freedman2015economics}.


\subsection{Challenges to Scientific Integrity}

The consequences of the reproducibility crisis extends beyond the academia to affect public trust in science, slow down the translation of research into practical applications, and potentially lead to the misallocation of substantial resources and the implementation of misinformed policies based on unreliable findings. The inability to reproduce preclinical research, for example, can significantly delay the development of therapies that are live saving, increase the pressure on already strained research budgets, and drive up the costs associated with drug development. The societal impacts are also significant, with misdirected effort, funding, and policies potentially being implemented based on research that cannot be validated \cite{freedman2015economics}.

Several interconnected factors contribute to this crisis, spanning issues within the publication system to the prevalence of questionable research practices and the inherent complexities encountered in certain scientific disciplines. Journals often exhibit a publication bias, preferentially publishing novel and positive results while overlooking negative findings or replication studies \cite{ioannidis2005most}. This creates a skewed representation of the scientific landscape and can lead to the neglect of important information about what does not work \cite{collins_policy_2014}. Furthermore, researchers may engage in questionable research practices, such as p-hacking (manipulating data to achieve statistical significance) and HARKing (hypothesizing after results are known), which can distort results and make replication exceedingly challenging. Inadequate statistical methods, including the use of suboptimal analyses, can also lead to erroneous conclusions, further hindering the replication process. A significant contributing factor is the lack of data sharing among researchers; when data and methods are not openly accessible, the ability of others to verify and replicate the work is severely limited \cite{munafo_manifesto_2017}.

The intense pressure to publish, often described by the expression "publish or perish," can incentivize researchers to prioritize the quantity of publications over their quality, potentially leading to rushed and less rigorous research. Incentive structures within universities may inadvertently reward the mere act of publication in prestigious journals, sometimes at the expense of methodological rigor and the pursuit of accurate and reproducible findings. This competitive environment can implicitly or explicitly encourage the use of questionable research practices to achieve publication, such as selectively reporting parts of datasets or trying different analytical approaches until the desired outcome is obtained \cite{david_robert_grimes_modelling_2018}.

The reproducibility crisis in science also reveals a strong connection between data management practices and the ability to replicate experimental results. Transparent and accessible data are essential for verifying findings and ensuring their reliability across disciplines. Insufficient metadata, unavailability of raw data, and incomplete methodological reporting are major contributors to irreproducibility. Without proper documentation and sharing protocols, researchers face significant barriers in reusing or validating published results \cite{samuel_understanding_2021}.

\subsection{A Paradigm Shift Towards Transparency and Collaboration}

In response to concerns about the reproducibility and reliability of scientific production, a movement emerged advocating for a fundamental transformation in how knowledge is generated and disseminated, emphasizing transparency, accessibility, and collaboration within the scientific community and with the broader public. Although the ideals of openness and sharing have long been embedded in scientific practice, the Open Science movement gained momentum with the advent of the internet and the more interactive capabilities made available by the Web 2.0 \cite{thibault_open_2023}.

\subsection{Open Science Principles as Solutions to the Reproducibility Crisis}

The Open Science practices are designed to confront reproducibility issues by promoting greater transparency, accessibility, and collaboration in scientific research. Among these practices, five core principles stand out: Open Data, Open Materials, Open Access, Preregistration, and Open Analysis. These principles address systemic issues that undermine the credibility and reliability of scientific outputs and seek to realign research practices with the foundational values of openness and verifiability \cite{van_dijk_open_2021}.

\begin{table}[ht]
    \centering
    \caption{The Five Principles of Open Science \cite{van_dijk_open_2021}}
    \label{tab:open_science_principles}
    \begin{tabular}{|l|p{11cm}|}
        \hline
        \textbf{Principle} & \textbf{Description}                                                                                                                                                 \\
        \hline
        Open Data          & Making research data freely available for others to inspect, reuse, and build upon, supporting transparency and reproducibility.                                     \\
        \hline
        Open Analysis      & Sharing code, workflows, and analysis scripts used in the study to allow others to verify and replicate the results.                                                 \\
        \hline
        Open Materials     & Providing full access to the materials, tools, and instruments used in the research, such as surveys, interventions, protocols or software.                          \\
        \hline
        Preregistration    & Publicly registering study designs, hypotheses, and analysis plans before data collection to prevent selective reporting and increase research integrity.            \\
        \hline
        Open Access        & Ensuring that research outputs, including publications, are freely accessible to all, removing barriers imposed by paywalls, subscritpions or restrictive licensing. \\
        \hline
    \end{tabular}
\end{table}

A central element of this framework is the commitment to Open Data, which calls for unrestricted access to raw research data and associated metadata. This principle directly addresses the lack of transparency that often impedes reproducibility by ensuring that the empirical foundation of research is available for validation, reinterpretation, and reuse. Open Data repositories serve a critical role in this ecosystem by preserving datasets in standardized formats, maintaining provenance metadata, and enabling persistent access. Provenance information about the origin, context, and transformations applied to the data is particularly important, as it supports reproducibility by providing a traceable record of how datasets were collected, processed, and interpreted. Without these metadata standards and traceability mechanisms, shared data risk becoming uninterpretable or misleading when repurposed \cite{learn_2017, burgelman_open_2019}.

Linked to Open Data is the principle of Open Materials, which involves making the research components such as experimental protocols, instructions and interventions. Open Materials ensure that researchers seeking to replicate a study or extend its methodology have access to the same inputs and tools used in the original work. Depositing these materials in domain-specific repositories and documenting them with clear metadata and provenance records enhances both transparency and usability \cite{van_dijk_open_2021}.

Open Access complements these practices by addressing the dissemination of research outputs. It entails making peer-reviewed publications freely available without subscription or payment barriers. Open Access expands the reach and impact of scientific knowledge, enabling researchers from under-resourced institutions and disciplines to participate in scholarly discourse and replication efforts. In conjunction with preprints—versions of manuscripts shared prior to peer review—Open Access accelerates the circulation of ideas and allows the broader community to scrutinize findings earlier in the research lifecycle. This early-stage visibility invites broader feedback and can help identify methodological flaws or inconsistencies that might otherwise go unnoticed until post-publication \cite{van_dijk_open_2021}.

To strengthen methodological transparency, Open Science also promotes Preregistration, which involves submitting a time-stamped outline of the research questions, hypotheses, and study design prior to data analysis. The adoption of preregistration discourages questionable research practices such as HARKing (Hypothesizing After the Results are Known) and p-hacking, thereby increasing transparency and reducing publication bias. This enhances the credibility of findings throughout the experimental process. Preregistered reports can be submitted to dedicated registries, assigned unique identifiers, and tracked by provenance systems that ensure the integrity and traceability of the research workflow \cite{van_dijk_open_2021}.

Finally, Open Analysis entails sharing the code and computational workflows used in data processing and statistical inference. By making analysis pipelines available, researchers allow others to reproduce exact outputs from shared data, supporting both validation and reuse. Integration with containerization tools, version control systems, and computational notebooks strengthens this principle, enabling complete provenance tracking of computational environments and decisions \cite{van_dijk_open_2021}.

Finally, Open Analysis involves the disclosure of code and computational workflows employed in data processing and statistical inference. By making analysis pipelines accessible, researchers enable others to reproduce the exact outputs from shared datasets, thereby facilitating both validation and reuse. The adoption of containerization tools, version control systems, and computational notebooks further reinforces this principle by enabling comprehensive provenance tracking of computational environments and analytical decisions \cite{van_dijk_open_2021, samuel_understanding_2021}.

Together, the five principles of Open Science—Open Data, Open Materials, Open Analysis, Preregistration, and Open Access form a cohesive approach to improving the reliability and transparency of scientific research. By promoting the use of open repositories, standardized metadata, and accessible workflows, these practices reshape how knowledge is produced and shared, fostering a more trustworthy and collaborative research environment.

\subsection{Current Initiatives and Standards for Enhancing Research Reproducibility}

\subsection{Key Initiatives in Open Science and Research Data Management}

The growing emphasis on transparency, reproducibility, and collaboration in scientific research has led to the emergence of several influential initiatives that support the implementation of Open Science and effective Research Data Management (RDM). These initiatives provide frameworks, tools, and community-driven guidelines that help researchers and institutions manage data more responsibly, ensuring that research outputs are not only preserved but also accessible and reusable. By fostering interoperability, encouraging FAIR (Findable, Accessible, Interoperable, and Reusable) data practices, and promoting a culture of openness, these efforts contribute to a more trustworthy and efficient research ecosystem. This section discusses a selection of leading initiatives spanning international collaborations, policy frameworks, and infrastructural developments that collectively shape the evolving landscape of Open Science and RDM.

\subsection{Key Initiatives in Research Data Management and Open Science}

\subsection{Leveraging European Research Data (LEARN)}
The LEARN Toolkit (Leveraging European Research Data) was developed to assist research institutions in implementing effective Research Data Management (RDM) policies and practices. Grounded in the recommendations of the LERU (League of European Research Universities) Roadmap for Research Data, the Toolkit offers guidance on institutional policy development, advocacy, training, infrastructure, and best practices. It emphasizes the strategic role of data management planning and encourages institutions to embed RDM into the research lifecycle. By providing a series of model policies, case studies, and checklists, LEARN promotes a culture of data stewardship aligned with the principles of FAIR data (Findable, Accessible, Interoperable, and Reusable), contributing to the broader objectives of Open Science \cite{learn_2017}.

\subsection{FAIR Guding Principles}
The FAIR Guiding Principles represents a cornerstone of responsible data stewardship in the context of Open Science. These principles aim to improve the infrastructure supporting the reuse of scholarly data. By encouraging data producers to make their outputs Findable, Accessible, Interoperable, and Reusable, FAIR fosters machine-readability, long-term preservation, and seamless data integration across platforms and disciplines. Although not inherently open, FAIR complements Open Science by providing the technical and semantic standards necessary for data sharing and reuse. Adoption of FAIR principles by research funders, repositories, and institutions has significantly influenced data policies across scientific communities and reinforced efforts toward more transparent and collaborative research practices \cite{wilkinson_fair_2016}.


\subsection{GO FAIR}
The GO FAIR initiative builds on the momentum of the FAIR principles, functioning as a bottom-up, stakeholder-driven movement to implement FAIR data stewardship globally. It encourages the development of implementation networks—collaborative groups that share expertise and develop domain-specific solutions for achieving FAIR data practices. GO FAIR’s focus extends to governance, education, and infrastructure, aiming to create a distributed ecosystem that facilitates the reuse of scientific data. By promoting interoperability standards and cultural change across the scientific community, GO FAIR advances Open Science by ensuring that data outputs can be seamlessly discovered, accessed, and reused across institutional and national boundaries \cite{henning_go_2019}.


\subsection{Research Data Alliance (RDA)}
The Research Data Alliance (RDA) is a global community-driven initiative that brings together data practitioners, technologists, and policymakers to build the social and technical infrastructure necessary for open data sharing across disciplines. Founded in 2013, RDA operates through working groups and interest groups that develop recommendations, standards, and best practices for data interoperability and stewardship. The RDA fosters international cooperation and bridges disciplinary gaps by aligning data governance, metadata standards, and infrastructure development. Its outputs support the implementation of Open Science by ensuring that research data is not only preserved but also rendered useful and actionable across diverse research contexts \cite{berman_research_2020}.

\subsection{Committee on Data of the International Science Council(CODATA)}
CODATA is an international organization committed to advancing data science and improving the quality and accessibility of research data. It plays a vital role in the global Open Science ecosystem by supporting the development of data policies, fostering international collaboration, and providing strategic guidance on data governance. CODATA actively contributes to the advancement of the FAIR principles and supports initiatives that aim to make research data a reusable, sustainable, and equitable public good. Through its coordination efforts and engagement with global stakeholders, CODATA helps shape the infrastructures and norms that underpin responsible data sharing and Open Science \cite{codata_2024}.

\subsection{Open Access Infrastructure for Research in Europe (OpenAIRE)}
OpenAIRE represents a pan-European initiative designed to support the open dissemination and reuse of research outputs. Originating as a response to the European Commission's Open Access policies, OpenAIRE has developed into a robust infrastructure that aggregates metadata and full-text content from a wide array of data providers, including institutional repositories, data archives, and scholarly journals. By facilitating interlinking between publications, datasets, software, and project information, OpenAIRE enhances the discoverability and interoperability of research products across disciplines. Its suite of services, such as the OpenAIRE Graph and Research Community Dashboards, provides tools for compliance monitoring, impact assessment, and reproducibility tracking. Furthermore, OpenAIRE actively contributes to policy development and technical alignment in the global Open Science ecosystem, advocating for standardized metadata schemas and persistent identifiers. Through its alignment with FAIR principles and support for the European Open Science Cloud (EOSC), OpenAIRE plays a foundational role in shaping a transparent, interconnected, and researcher-centric data landscape \cite{rettberg_openaire_2012}.

\subsection{DataCite}

DataCite is a global non-profit organization that plays a foundational role in the research data ecosystem by providing persistent identifiers—most notably Digital Object Identifiers (DOIs)—for datasets and other research outputs. Founded to support data citation practices, DataCite promotes the discoverability, accessibility, and reuse of research data by ensuring that data can be persistently linked to scholarly publications and contributors. It collaborates with data centers, publishers, and repositories to establish metadata standards that facilitate interoperability across infrastructures. Through services such as DOI registration, metadata management, and citation tracking, DataCite actively contributes to the implementation of the FAIR principles and strengthens the overall architecture of Open Science and Research Data Management worldwide \cite{brase_datacite_2009}.

\subsection{Nelson Memo - Office of Science and Technology Policy (OSTP)}

In 2022, the White House Office of Science and Technology Policy (OSTP) issued a directive known as the “Nelson Memo,” which requires that all federally funded research publications and associated data be made immediately and freely available to the public by December 31, 2025. This policy marks a pivotal shift in U.S. open access strategy by eliminating embargo periods and strengthening mandates for data transparency. Building on previous open science policies, the Nelson Memo seeks to ensure equitable access to publicly funded knowledge, drive reproducibility, and accelerate scientific progress through a national commitment to openness and accountability \cite{nelson_2023}.


\begin{table}[H]
    \centering
    \caption{Comparison of Open Science Related Initiatives}
    \label{tab:initiative_comparison}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|p{3.5cm}|p{4cm}|p{5cm}|p{5cm}|}
            \hline
            \textbf{Initiative}                & \textbf{Coverage}                           & \textbf{Key Outputs}                                                        & \textbf{Contribution to Open Science}                                                        \\
            \hline
            LEARN                              & Institutional; Europe (globally applicable) & RDM policy toolkit, model policies, case studies                            & Strengthens institutional capacity for implementing FAIR and Open Data policies              \\
            \hline
            FAIR Guiding Principles            & European Union                              & FAIR assessment tools, training materials, recommendations                  & Embeds FAIR principles into research workflows and infrastructures                           \\
            \hline
            GO FAIR                            & Global                                      & FAIRification framework, implementation networks, training modules          & Operationalizes FAIR principles through community-driven practices                           \\
            \hline
            EOSC (European Open Science Cloud) & Pan-European Infrastructure                 & EOSC Portal, service registry, metadata standards                           & Provides federated infrastructure to enable Open Science practices across disciplines        \\
            \hline
            RDA (Research Data Alliance)       & Global                                      & Working group outputs, interoperability guidelines, standards               & Enhances technical and social infrastructure for global data sharing                         \\
            \hline
            CODATA                             & Global (UNESCO)                             & Policy frameworks, capacity-building initiatives, data science standards    & Supports Open Science through coordination of global data policy and governance              \\
            \hline
            OpenAIRE                           & European Union                              & Research Graph, repository integration tools, metadata guidelines           & Connects RDM and Open Access via aggregated infrastructure and metadata interoperability     \\
            \hline
            DataCite                           & Global                                      & DOI registration service, metadata schema, discovery APIs, DataCite Commons & Enables FAIR data by ensuring traceability, citation, and persistent access in open research \\
            \hline
            OSTP Nelson Memo                   & United States / Federal Policy              & Mandate immediate public access to federally funded research outputs        & Shapes U.S. policy landscape for Open Access and data sharing by 2026                        \\
            \hline
        \end{tabular}%
    }
\end{table}


\section*{Decentralized Applications in Support of Open Science and Reproducibility}

The limitations of traditional research data management systems have sparked growing interest in alternative models. Decentralized technologies, particularly blockchain, have gained increasing recognition for their ability to enhance transparency, accountability, and trust across various domains, including scientific research. Their potential to address long-standing inefficiencies and structural shortcomings within the research ecosystem has attracted significant attention from the academic community.

Blockchain has evolved into a broader paradigm of distributed ledger technology, collectively maintained by a network of nodes. Through immutability and consensus-based validation, it ensures the integrity of recorded data. These foundational features offer a technological infrastructure for verifying the authenticity, provenance, and persistence of digital records, features that align closely with Open Science objectives and the FAIR principles (Findable, Accessible, Interoperable, and Reusable). In an era of data-intensive research and multi-stakeholder collaboration, such assurances are critical for enabling reproducibility, facilitating the auditability of research processes, and ensuring reliable attribution of intellectual contributions.

Decentralized solutions introduce a novel approach to scientific data governance. This paradigm shift directly supports key principles of Open Science such as openness, inclusivity, reproducibility, and collaboration by embedding accountability and traceability into the technical fabric of research infrastructures.

In this context, decentralized applications function as strategic enablers of both cultural and procedural transformation in science. They offer pathways to reconfigure incentive structures, reduce access barriers, and reinforce the reproducibility and credibility of scientific outputs. The remainder of this section explores the current state of such applications, their underlying architectures, and the roles they play in advancing Open Science and addressing reproducibility challenges.

\cite{lawlor_overview_2018} provided an early synthesis of the 2018 NFAIS conference on blockchain in scholarly publishing, emphasizing its potential to reshape research workflows, peer review, and intellectual property management. The article discussed several pilot initiatives—such as ARTiFACTS and Knowbella Tech—that exemplify blockchain’s promise for enhancing provenance tracking and decentralizing research funding. While the enthusiasm for blockchain's role in creating secure, transparent, and decentralized scholarly infrastructures was evident, the article also noted that broad adoption hinges on increased awareness and a more nuanced understanding of the technology's capabilities..

\cite{holmen_blockchain_2018} explores how blockchain could help decentralize scholarly publishing by unbundling traditional workflows and empowering content creators through more equitable recognition and compensation mechanisms. By drawing attention to token-based platforms like Steem, BAT, and LBRY, the article illustrates how blockchain could challenge the dominance of centralized content discovery systems and offer direct value to researchers. A central argument is that focusing on creators' needs—particularly in terms of discoverability and monetization—could lead to more effective publishing ecosystems. While optimistic about blockchain’s potential to reduce delivery-chain costs and enhance transparency, the article also recognizes that such a shift would likely require rethinking current revenue models.

\cite{kochalko_making_2019} presents blockchain as a transformative tool capable of advancing Eugene Garfield’s vision of comprehensive recognition for all research contributions, especially those outside traditional publishing channels. Emphasizing platforms like ARTiFACTS, the article illustrates how blockchain can secure the provenance and attribution of scholarly outputs from the earliest stages of the research lifecycle. By enabling researchers to establish formal recognition for pre-published work, blockchain technologies promise to reshape the incentive structures in academia and elevate the visibility of diverse forms of scholarly activity. The piece conveys a long-term perspective in which blockchain becomes a foundational infrastructure for open science and academic reputation systems.

\cite{van_rossum_blockchain_2018} presents a broad and forward-looking exploration of how blockchain technology could address systemic inefficiencies and credibility issues within science and scholarly publishing. The article underscores blockchain’s potential to reform core academic processes such as peer review and reproducibility, while also opening novel avenues for research funding through cryptocurrency-based incentives. It introduces concepts like decentralized repositories and micropayment systems that could transform access, attribution, and reward mechanisms in academia. Notably, the article frames blockchain as an enabler of decentralized digital rights management and improved research metrics, capable of supporting a more transparent and equitable scholarly ecosystem. However, the author also notes that meaningful adoption will require overcoming significant inertia embedded in existing institutional frameworks and cultural practices.

\cite{gazis_blockchain_2022} in this article proposes a blockchain-based cloud middleware framework aimed at improving academic manuscript submission and peer-review processes. Rather than suggesting a complete overhaul of existing systems, the authors emphasize integration with current infrastructures to enhance anonymity, reduce bias, and increase decentralization. The paper presents a four-tier middleware architecture and a reviewer selection algorithm, both designed to optimize the peer-review workflow. Utilizing open-source tools such as Java Spring and the Ethereum blockchain, the proposed framework demonstrates potential for creating a more privacy-focused and decentralized submission system. Preliminary testing with simulated data supports the framework’s effectiveness, although the authors acknowledge challenges related to real-world scalability and implementation.

\cite{leible_review_2019} in this article offers a systematic review of blockchain’s application to open science, highlighting the alignment between blockchain’s core features—decentralization, immutability, and transparency—and the principles of open science. Through the categorization of 60 blockchain-based projects, the article provides a comprehensive overview of the field as it stood in 2019, identifying trends such as efforts to improve reproducibility, enable secure resource sharing, and support intellectual property protection. The authors also compare blockchain’s technical characteristics with the infrastructural needs of open science ecosystems, emphasizing both the promise and the complexity of this integration. Key challenges identified include the absence of standardization, the technical risks of deploying smart contracts, and the difficulty of designing sustainable incentive models. While the review concludes that blockchain holds considerable promise as a foundation for open science, it stresses that realizing this potential requires addressing these limitations and achieving broader acceptance across the scientific community.

\cite{trovo_ants-review_2021} published as a chapter in Euro-Par 2020: Parallel Processing Workshops (2021), this work addresses the persistent lack of incentives for scientists to engage in peer review by introducing Ants-Review, a blockchain-based protocol designed to support anonymous yet rewarded evaluations using smart contracts on the Ethereum platform (Ants-Review, 2021). The system allows authors to offer bounties in a native token, ANTS, for reviews that satisfy predefined quality standards. To address privacy concerns—critical to wider academic adoption—the protocol incorporates the AZTEC Protocol to ensure anonymity. A gamified community mechanism enables broader participation in evaluating and voting on the quality of peer reviews, thereby promoting ethical behavior and inclusivity. The authors describe core components such as access control, tokenomics, and privacy enforcement, and envision future integrations with Decentralized Finance (DeFi) services and governance through a Decentralized Autonomous Organization (DAO), offering a comprehensive approach to enhancing fairness and efficiency in scientific publishing.

\cite{kosmarski_blockchain_2020} A 2020 article in the Journal of Open Innovation explores how blockchain could enhance open data sharing, peer review, research funding, and governance in academia, while identifying key adoption barriers such as usability challenges, legal uncertainty, and value conflicts between decentralization and academic norms (10.3390/joitmc6040117). In a 2021 Learned Publishing article, the authors examine the potential of blockchain to improve scholarly journal evaluation by providing a transparent, tamper-proof system for tracking metrics like citations and usage, which could increase trust in journal credibility.

\cite{putnings_non-fungible_2022} explores the potential of NFTs to restore a sense of unique ownership and value to digital scholarly works. It proposes science-friendly integration pathways through university presses, submission platforms, and DOI agencies, while emphasizing essential requirements such as cost-free generation and transfer, researcher autonomy, interoperability, and low technical complexity. In a complementary vein, the article “Open Lab: A web application for running and sharing online experiments” (10.3758/s13428-021-01776-2), published in *Behavior Research Methods* (2022), introduces *Open Lab*, a browser-based platform for deploying and sharing experiments created with lab.js. The platform simplifies online research by offering tools for participant management, flexible randomization, and data export, while integrating with the Open Science Framework to support transparency, reproducibility, and open collaboration across the research community.

\cite{zhou_open-pub_2021} this introduces Open-Pub, a blockchain-based academic publishing system designed to balance the seemingly conflicting goals of transparency and privacy. The authors critique current academic publishing models for their limitations in openness, integrity, and data control, proposing a private Ethereum-based blockchain as a viable alternative. A central innovation is the use of a threshold identity-based group signature scheme, which enables secure, anonymous peer review while still ensuring accountability. The system also leverages the InterPlanetary File System (IPFS) for decentralized, persistent storage of both manuscripts and review reports, thereby enhancing accessibility and data integrity. Additionally, *Open-Pub* introduces a token-based incentive mechanism to reward reviewers, reinforcing community engagement and improving the peer review process. Although the paper does not elaborate extensively on incentives for authors, it outlines a comprehensive architecture and feature set aimed at fostering a more equitable and privacy-conscious scholarly communication model. The development of *Open-Pub* exemplifies the growing interest in blockchain technologies to reimagine academic publishing infrastructures, particularly in contexts requiring both control and openness.

\cite{lee_unblocking_2023} critiques the inadequacies of current academic recognition systems, which often privilege publications and citations while neglecting other vital academic contributions. It proposes a blockchain-backed token system that rewards a broader range of academic activities—such as peer review, committee participation, and report submission—with non-tradable, non-monetisable tokens. These tokens serve as a transparent and validated record of contributions, aiming to enhance professional assessments and incentivize engagement in essential but frequently overlooked academic tasks. Designed within the context of the National Institute for Health and Care Research (NIHR), the system is positioned to improve recognition for academic service, increase efficiency in peer review and reporting, and foster cross-funder collaboration. While highlighting significant benefits, the article also acknowledges potential challenges, including public skepticism of blockchain, data protection concerns, regulatory compliance, and the need to ensure token value within academic cultures. The article fits into broader trends across scholarly publishing literature, where blockchain is increasingly seen as a means to improve peer review, incentivize data sharing, enable better author attribution, and support the decentralization and transparency of publishing processes. Themes such as the use of NFTs for digital academic assets, token systems for incentivization, and blockchain-backed models for open science are prominent, alongside debates over usability, governance, legal frameworks, and the role of traditional publishers. The shift from conceptual models to more concrete implementations and critical evaluations reflects a maturing research area that is moving beyond hype to consider the practical application and integration of blockchain into scholarly communication.

\todo{Conclusion}

In conclusion, the body of research reviewed in this section illustrates how blockchain technology can serve as a foundational enabler not only for reforming scholarly publishing but also for advancing the broader goals of Open Science and addressing enduring concerns around reproducibility. The studies highlight a range of blockchain applications, from enhancing the transparency and accountability of peer review processes to enabling novel systems of contributor recognition and facilitating the secure management of digital research assets. These innovations are closely aligned with the principles of Open Science, including openness, accessibility, interoperability, and the traceability of scientific processes. Beyond the publishing lifecycle, blockchain offers mechanisms to strengthen research integrity by ensuring provenance, versioning, and tamper-proof audit trails for data and workflows, all of which are critical for improving reproducibility in scientific research. Nonetheless, the transition from conceptual promise to systemic adoption requires overcoming substantial barriers related to technological maturity, governance, interoperability with existing infrastructure, and user acceptance across diverse research communities. Moving forward, the path toward meaningful integration of blockchain into scholarly communication and Open Science will depend on collaborative, interdisciplinary efforts aimed at designing user-oriented solutions that address real-world research challenges. A pragmatic and critical perspective—one that embraces blockchain's potential while remaining attentive to its limitations—will be essential for harnessing the technology to build a more trustworthy, transparent, and inclusive research ecosystem.


\bibliographystyle{plain}
\bibliography{Bibliography.bib}


\end{document}


