{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193746bd-1554-4a89-88d9-f0fde02c3b0d",
   "metadata": {},
   "source": [
    "IPFS Search Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c4aee-6c51-4d9a-be31-e4f8782a9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipfshttpclient\n",
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "from tika import parser\n",
    "from cuckoo_filter import CuckooFilter\n",
    "\n",
    "# Initialize IPFS client to connect to the local IPFS node running in Docker\n",
    "client = ipfshttpclient.connect('/ip4/127.0.0.1/tcp/5001')\n",
    "\n",
    "# Initialize Cuckoo filter for cache\n",
    "cache_filter = CuckooFilter(capacity=1000, bucket_size=4)\n",
    "\n",
    "# Function to extract metadata and keywords using Apache Tika\n",
    "def extract_metadata(file_path):\n",
    "    try:\n",
    "        parsed = parser.from_file(file_path)\n",
    "        metadata = parsed.get('metadata', {})\n",
    "        content = parsed.get('content', '')\n",
    "        \n",
    "        # Simplified keyword extraction: split content into keywords\n",
    "        # You might want to use more sophisticated NLP techniques here\n",
    "        keywords = [word.lower() for word in content.split() if len(word) > 3]  # Filter short words\n",
    "        \n",
    "        cid = add_file_to_ipfs(file_path)\n",
    "        if cid:\n",
    "            return {\"CID\": cid, \"metadata\": metadata, \"keywords\": keywords}\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata with Tika: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to add a file to IPFS and return its CID\n",
    "def add_file_to_ipfs(file_path):\n",
    "    try:\n",
    "        result = client.add(file_path)\n",
    "        print(f\"File added to IPFS with CID: {result['Hash']}\")\n",
    "        return result['Hash']\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding file to IPFS: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to add metadata to the DHT\n",
    "def add_to_dht(metadata):\n",
    "    for keyword in metadata[\"keywords\"]:\n",
    "        key = hashlib.sha256(keyword.encode()).hexdigest()\n",
    "        cid_list = dht_get(key) or []  # Fetch existing CIDs from DHT\n",
    "        if metadata[\"CID\"] and metadata[\"CID\"] not in cid_list:\n",
    "            cid_list.append(metadata[\"CID\"])\n",
    "        dht_put(key, cid_list)  # Store updated CID list in DHT\n",
    "\n",
    "# Placeholder functions for DHT interaction\n",
    "def dht_put(key, value):\n",
    "    # Use IPFS commands to interact with the DHT (this may require custom implementation)\n",
    "    # Currently, IPFS does not have direct DHT API calls available in python bindings.\n",
    "    print(f\"Putting {key}: {value} into DHT (pseudo-code).\")\n",
    "\n",
    "def dht_get(key):\n",
    "    # Use IPFS commands to retrieve from DHT\n",
    "    print(f\"Getting {key} from DHT (pseudo-code).\")\n",
    "    return []\n",
    "\n",
    "# Function to search in DHT with cache check\n",
    "def search_in_dht(keywords):\n",
    "    canonical_form = sorted(keywords)  # Simple lexical ordering\n",
    "    query_key = hashlib.sha256(' '.join(canonical_form).encode()).hexdigest()\n",
    "\n",
    "    # Check cache first\n",
    "    if cache_filter.contains(query_key):\n",
    "        print(\"Cache hit for:\", query_key)\n",
    "        # Fetch from cache (not implemented in this pseudocode)\n",
    "        return\n",
    "    \n",
    "    # Perform DHT search if not in cache\n",
    "    results = []\n",
    "    for keyword in keywords:\n",
    "        key = hashlib.sha256(keyword.encode()).hexdigest()\n",
    "        cids = dht_get(key)\n",
    "        results.append(set(cids))\n",
    "    \n",
    "    # Intersection of sets for conjunctive query\n",
    "    final_results = set.intersection(*results) if results else set()\n",
    "    \n",
    "    # Update cache\n",
    "    cache_filter.insert(query_key)\n",
    "    print(\"Search results:\", final_results)\n",
    "\n",
    "# Example usage\n",
    "file_path = \"path/to/your/document.pdf\"\n",
    "metadata = extract_metadata(file_path)\n",
    "if metadata:\n",
    "    add_to_dht(metadata)\n",
    "    search_in_dht([\"example\", \"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fbe68-47d5-4949-9247-f268f8d06d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "To integrate Apache Tika for metadata extraction and keyword processing, you'll need to use Tika's Python library, which allows you to parse various types of documents (PDFs, Word files, etc.) and extract relevant metadata. Below, I'll update the code to include the use of Apache Tika for extracting metadata, and then save this data to IPFS, all while connecting to an IPFS node running on Docker.\n",
    "\n",
    "### Step-by-Step Updated Code\n",
    "\n",
    "1. **Run IPFS in Docker**: Use the previous Docker setup to ensure your IPFS node is running locally.\n",
    "2. **Use Apache Tika for Metadata Extraction**: Leverage the Tika library to extract keywords and metadata from files.\n",
    "3. **Store Extracted Metadata and Keywords in IPFS**: Use IPFS to store and index the data.\n",
    "4. **Interact with IPFS DHT and Cache**: Utilize IPFS DHT to store keyword-to-CID mappings and Cuckoo filters for caching results.\n",
    "\n",
    "### Updated Python Code\n",
    "\n",
    "Here's a comprehensive implementation that incorporates Apache Tika for metadata extraction:\n",
    "\n",
    "```python\n",
    "import ipfshttpclient\n",
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "from tika import parser\n",
    "from cuckoo_filter import CuckooFilter\n",
    "\n",
    "# Initialize IPFS client to connect to the local IPFS node running in Docker\n",
    "client = ipfshttpclient.connect('/ip4/127.0.0.1/tcp/5001')\n",
    "\n",
    "# Initialize Cuckoo filter for cache\n",
    "cache_filter = CuckooFilter(capacity=1000, bucket_size=4)\n",
    "\n",
    "# Function to extract metadata and keywords using Apache Tika\n",
    "def extract_metadata(file_path):\n",
    "    try:\n",
    "        parsed = parser.from_file(file_path)\n",
    "        metadata = parsed.get('metadata', {})\n",
    "        content = parsed.get('content', '')\n",
    "        \n",
    "        # Simplified keyword extraction: split content into keywords\n",
    "        # You might want to use more sophisticated NLP techniques here\n",
    "        keywords = [word.lower() for word in content.split() if len(word) > 3]  # Filter short words\n",
    "        \n",
    "        cid = add_file_to_ipfs(file_path)\n",
    "        if cid:\n",
    "            return {\"CID\": cid, \"metadata\": metadata, \"keywords\": keywords}\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata with Tika: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to add a file to IPFS and return its CID\n",
    "def add_file_to_ipfs(file_path):\n",
    "    try:\n",
    "        result = client.add(file_path)\n",
    "        print(f\"File added to IPFS with CID: {result['Hash']}\")\n",
    "        return result['Hash']\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding file to IPFS: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to add metadata to the DHT\n",
    "def add_to_dht(metadata):\n",
    "    for keyword in metadata[\"keywords\"]:\n",
    "        key = hashlib.sha256(keyword.encode()).hexdigest()\n",
    "        cid_list = dht_get(key) or []  # Fetch existing CIDs from DHT\n",
    "        if metadata[\"CID\"] and metadata[\"CID\"] not in cid_list:\n",
    "            cid_list.append(metadata[\"CID\"])\n",
    "        dht_put(key, cid_list)  # Store updated CID list in DHT\n",
    "\n",
    "# Placeholder functions for DHT interaction\n",
    "def dht_put(key, value):\n",
    "    # Use IPFS commands to interact with the DHT (this may require custom implementation)\n",
    "    # Currently, IPFS does not have direct DHT API calls available in python bindings.\n",
    "    print(f\"Putting {key}: {value} into DHT (pseudo-code).\")\n",
    "\n",
    "def dht_get(key):\n",
    "    # Use IPFS commands to retrieve from DHT\n",
    "    print(f\"Getting {key} from DHT (pseudo-code).\")\n",
    "    return []\n",
    "\n",
    "# Function to search in DHT with cache check\n",
    "def search_in_dht(keywords):\n",
    "    canonical_form = sorted(keywords)  # Simple lexical ordering\n",
    "    query_key = hashlib.sha256(' '.join(canonical_form).encode()).hexdigest()\n",
    "\n",
    "    # Check cache first\n",
    "    if cache_filter.contains(query_key):\n",
    "        print(\"Cache hit for:\", query_key)\n",
    "        # Fetch from cache (not implemented in this pseudocode)\n",
    "        return\n",
    "    \n",
    "    # Perform DHT search if not in cache\n",
    "    results = []\n",
    "    for keyword in keywords:\n",
    "        key = hashlib.sha256(keyword.encode()).hexdigest()\n",
    "        cids = dht_get(key)\n",
    "        results.append(set(cids))\n",
    "    \n",
    "    # Intersection of sets for conjunctive query\n",
    "    final_results = set.intersection(*results) if results else set()\n",
    "    \n",
    "    # Update cache\n",
    "    cache_filter.insert(query_key)\n",
    "    print(\"Search results:\", final_results)\n",
    "\n",
    "# Example usage\n",
    "file_path = \"path/to/your/document.pdf\"\n",
    "metadata = extract_metadata(file_path)\n",
    "if metadata:\n",
    "    add_to_dht(metadata)\n",
    "    search_in_dht([\"example\", \"test\"])\n",
    "```\n",
    "\n",
    "### Key Components of the Updated Code:\n",
    "1. **Metadata Extraction with Apache Tika**: Uses Tika to parse documents and extract metadata and keywords. This is critical for populating the search index in the IPFS DHT.\n",
    "2. **Adding Files to IPFS**: Adds files to the IPFS network and retrieves the CID, which is then used in the indexing process.\n",
    "3. **DHT Storage and Retrieval**: Implements basic functions for storing and retrieving data from the IPFS DHT. Note that direct interaction with DHT using Python bindings may require additional configurations or tools.\n",
    "4. **Caching with Cuckoo Filters**: Implements caching of query results using a Cuckoo filter to speed up search operations and reduce redundant DHT queries.\n",
    "\n",
    "### Additional Considerations:\n",
    "- **IPFS DHT Interactions**: The IPFS Python client (`ipfshttpclient`) does not directly support DHT manipulation, so you might need to use IPFS CLI commands or extend the functionality with custom scripts.\n",
    "- **Advanced Keyword Extraction**: For better keyword extraction, consider using Natural Language Processing (NLP) techniques or libraries like NLTK or spaCy.\n",
    "- **Security and Robustness**: Ensure that the Docker setup is secure and robust for your deployment environment, especially if this setup is meant to scale or handle sensitive data.\n",
    "\n",
    "Let me know if you need further adjustments or additional functionalities!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
