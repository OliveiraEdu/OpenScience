{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11965914-6a6a-43b4-a793-01a3eda28617",
   "metadata": {},
   "source": [
    "# Artifact - Project Account Detail - JSON Extraction with Apache Tika and IPFS Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f965815-8625-49cb-9627-267ef1a19069",
   "metadata": {},
   "source": [
    "** For requirements and initial setup go to https://github.com/OliveiraEdu/OpenScience/Readme.md **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b074d9-3edc-410d-9972-222d9cbb551f",
   "metadata": {},
   "source": [
    "1.1 -  Contract Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd2c53-d314-48c4-97d4-30925cdb52a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Crypto.Hash import keccak\n",
    "import os\n",
    "import binascii\n",
    "from iroha import IrohaCrypto\n",
    "from iroha import Iroha, IrohaGrpc\n",
    "from iroha.ed25519 import H\n",
    "import integration_helpers\n",
    "from iroha.primitive_pb2 import can_set_my_account_detail\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 or a more recent version is required.\")\n",
    "\n",
    "# Load configuration from config.json file\n",
    "config_path = \"config.json\"  # Update this path as needed\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "IROHA_HOST_ADDR = config[\"IROHA_HOST_ADDR\"]\n",
    "IROHA_PORT = config[\"IROHA_PORT\"]\n",
    "ADMIN_ACCOUNT_ID = config[\"ADMIN_ACCOUNT_ID\"]\n",
    "ADMIN_PRIVATE_KEY = config[\"ADMIN_PRIVATE_KEY\"]\n",
    "\n",
    "iroha = Iroha(ADMIN_ACCOUNT_ID)\n",
    "net = IrohaGrpc(\"{}:{}\".format(IROHA_HOST_ADDR, IROHA_PORT))\n",
    "\n",
    "user_private_key = IrohaCrypto.private_key()\n",
    "user_public_key = IrohaCrypto.derive_public_key(user_private_key)\n",
    "\n",
    "# Read account attributes from a csv\n",
    "def read_accounts_from_csv(file_path):\n",
    "    accounts = []\n",
    "    with open(file_path, mode='r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        for row in csv_reader:\n",
    "            accounts.append({\n",
    "                'account_id': row['project_id']\n",
    "            })\n",
    "    return accounts\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'datasets/projects.csv'\n",
    "\n",
    "# Read accounts from CSV\n",
    "accounts = read_accounts_from_csv(csv_file_path)\n",
    "\n",
    "# Use the [n] account from the CSV for the example\n",
    "account = accounts[6]\n",
    "\n",
    "@integration_helpers.trace\n",
    "def create_contract():\n",
    "    bytecode = \"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\"\n",
    "    \"\"\"Bytecode was generated using remix editor  https://remix.ethereum.org/ from file detail.sol. \"\"\"\n",
    "    tx = iroha.transaction(\n",
    "        [iroha.command(\"CallEngine\", caller=ADMIN_ACCOUNT_ID, input=bytecode)]\n",
    "    )\n",
    "    IrohaCrypto.sign_transaction(tx, ADMIN_PRIVATE_KEY)\n",
    "    net.send_tx(tx)\n",
    "    hex_hash = binascii.hexlify(IrohaCrypto.hash(tx))\n",
    "    for status in net.tx_status_stream(tx):\n",
    "        print(status)\n",
    "    return hex_hash\n",
    "\n",
    "\n",
    "@integration_helpers.trace\n",
    "def set_account_detail(address, account, variable_1, variable_2):\n",
    "    params = integration_helpers.get_first_four_bytes_of_keccak(\n",
    "        b\"setAccountDetail(string,string,string)\"\n",
    "    )\n",
    "    no_of_param = 3\n",
    "    for x in range(no_of_param):\n",
    "        params = params + integration_helpers.left_padded_address_of_param(\n",
    "            x, no_of_param\n",
    "        )\n",
    "    params = params + integration_helpers.argument_encoding(\n",
    "        account['account_id']\n",
    "    )  # source account id\n",
    "    params = params + integration_helpers.argument_encoding(variable_1)  # key\n",
    "    params = params + integration_helpers.argument_encoding(variable_2)  #  value\n",
    "    tx = iroha.transaction(\n",
    "        [\n",
    "            iroha.command(\n",
    "                \"CallEngine\", caller=ADMIN_ACCOUNT_ID, callee=address, input=params\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    IrohaCrypto.sign_transaction(tx, ADMIN_PRIVATE_KEY)\n",
    "    response = net.send_tx(tx)\n",
    "    print(response)\n",
    "    for status in net.tx_status_stream(tx):\n",
    "        print(status)\n",
    "    hex_hash = binascii.hexlify(IrohaCrypto.hash(tx))\n",
    "    return hex_hash\n",
    "\n",
    "\n",
    "@integration_helpers.trace\n",
    "def get_account_details():\n",
    "    params = integration_helpers.get_first_four_bytes_of_keccak(b\"getAccountDetail()\")\n",
    "    no_of_param = 0\n",
    "    tx = iroha.transaction(\n",
    "        [\n",
    "            iroha.command(\n",
    "                \"CallEngine\", caller=ADMIN_ACCOUNT_ID, callee=address, input=params\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    IrohaCrypto.sign_transaction(tx, ADMIN_PRIVATE_KEY)\n",
    "    response = net.send_tx(tx)\n",
    "    for status in net.tx_status_stream(tx):\n",
    "        print(status)\n",
    "    hex_hash = binascii.hexlify(IrohaCrypto.hash(tx))\n",
    "    return hex_hash\n",
    "\n",
    "\n",
    "hash = create_contract()\n",
    "address = integration_helpers.get_engine_receipts_address(hash)\n",
    "integration_helpers.get_engine_receipts_result(hash)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931dbcd1-80b5-43a4-a855-4ddba9083450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query - GetAccountDetail\n",
    "query = iroha.query('GetAccountDetail',account_id=account['account_id'])\n",
    "IrohaCrypto.sign_query(query, ADMIN_PRIVATE_KEY)\n",
    "response = net.send_query(query)\n",
    "data = response.account_detail_response\n",
    "print(f'Account id = {account}, details = {data.detail}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea1051-cb1c-4c29-97e5-38ba90248081",
   "metadata": {},
   "source": [
    "1.2 -  Account Detail Setting\n",
    "\n",
    "Extracts metadata info from 'assets.json' and injects it as account details, calling function `set_account_details`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6343e-7a08-4e24-90fe-7aaf22bdc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For file handling\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tika import parser\n",
    "from ipfs_functions import *\n",
    "import icecream as ic\n",
    "\n",
    "def parse_documents_in_directory(directory_path):\n",
    "    \n",
    "    index = 1\n",
    "\n",
    "    print(account['account_id'])\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "\n",
    "        # Block 1\n",
    "        # Skip hidden files by checking if it starts with a dot\n",
    "        if not os.path.basename(filename).startswith('.'):\n",
    "            \n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(file_path)\n",
    "            file_cid = upload_file_to_ipfs(file_path)\n",
    "               \n",
    "            variable_1 = f\"file_{index}_CID\"\n",
    "            print(variable_1)\n",
    "            \n",
    "            variable_2 = file_cid\n",
    "            print (variable_2)\n",
    "\n",
    "            hash = set_account_detail(address, account, variable_1, variable_2)\n",
    "            \n",
    "        # Block 2\n",
    "        # Check if it's a file and not a directory\n",
    "        # if not os.path.basename(filename).startswith('.'):\n",
    "            try:\n",
    "                parsed_document = parser.from_file(file_path)\n",
    "           \n",
    "                \n",
    "                # Check if parsing was successful\n",
    "                if 'status' in parsed_document and parsed_document['status'] == 200:\n",
    "                    metadata = parsed_document.get('metadata', {})\n",
    "                    # print(metadata)\n",
    "                    metadata_cid = upload_json_to_ipfs(metadata)\n",
    "                    \n",
    "                    variable_3 = f\"file_{index}_metadata_CID\"\n",
    "                    print (variable_3)\n",
    "\n",
    "                    variable_4 = metadata_cid\n",
    "                    print (variable_4)\n",
    "\n",
    "                    hash = set_account_detail(address, account, variable_3, variable_4)\n",
    "                    \n",
    "     \n",
    "                    \n",
    "                else:\n",
    "                    print(f\"Parsing failed for '{filename}' with status: {parsed_document.get('status')}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred with file '{filename}': {e}\")\n",
    "            \n",
    "            print (\"-\" * 40)\n",
    "            index += 1\n",
    "\n",
    "# Example usage\n",
    "parse_documents_in_directory(\"upload\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba013f9-7085-48bb-8fdd-4a7ca18c38dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of dictionaries:\n",
      "  title: Assessing the Impact of Urbanization on Avian Migration Patterns\n",
      "  abstract: This study aims to investigate the effects of urbanization on avian migration patterns and identify potential conservation strategies.\n",
      "  keywords: ['urbanization', 'avian migration', 'conservation']\n",
      "  start_date: 2020-01-01\n",
      "  end_date: 2022-12-31\n",
      "  funding_agency: National Science Foundation\n",
      "  location: San Francisco Bay Area, California, USA\n",
      "  title: Investigating the Effects of Climate Change on Coral Reef Ecosystems\n",
      "  abstract: This study aims to investigate the impacts of climate change on coral reef ecosystems and identify potential mitigation strategies.\n",
      "  keywords: ['climate change', 'coral reefs', 'ecosystem resilience']\n",
      "  start_date: 2019-06-01\n",
      "  end_date: 2021-05-31\n",
      "  funding_agency: National Oceanic and Atmospheric Administration\n",
      "  location: Great Barrier Reef, Australia\n",
      "  title: Understanding the Role of Microplastics in Marine Food Webs\n",
      "  abstract: This study aims to investigate the impacts of microplastics on marine food webs and identify potential strategies for reducing plastic pollution.\n",
      "  keywords: ['microplastics', 'marine food webs', 'plastic pollution']\n",
      "  start_date: 2018-03-01\n",
      "  end_date: 2020-02-28\n",
      "  funding_agency: Environmental Protection Agency\n",
      "  location: Gulf of Mexico, USA\n",
      "  title: Developing Sustainable Agriculture Practices for Small-Scale Farmers\n",
      "  abstract: This study aims to develop and evaluate sustainable agriculture practices for small-scale farmers in developing countries.\n",
      "  keywords: ['sustainable agriculture', 'small-scale farming', 'developing countries']\n",
      "  start_date: 2017-09-01\n",
      "  end_date: 2019-08-31\n",
      "  funding_agency: Bill and Melinda Gates Foundation\n",
      "  location: Rwanda, Africa\n",
      "  title: Investigating the Effects of Climate Change on Coral Reef Ecosystems\n",
      "  abstract: This study aims to investigate the impacts of climate change on coral reef ecosystems and identify potential management strategies.\n",
      "  keywords: ['climate change', 'coral reefs', 'ecosystem management']\n",
      "  start_date: 2019-06-01\n",
      "  end_date: 2021-05-31\n",
      "  funding_agency: National Oceanic and Atmospheric Administration (NOAA)\n",
      "  location: Hawaii, USA\n",
      "  title: Examining the Relationship Between Microplastics and Marine Life\n",
      "  abstract: This study aims to investigate the effects of microplastics on marine life and identify potential strategies for reducing plastic pollution.\n",
      "  keywords: ['microplastics', 'marine life', 'plastic pollution']\n",
      "  start_date: 2018-01-01\n",
      "  end_date: 2020-12-31\n",
      "  funding_agency: Environmental Protection Agency (EPA)\n",
      "  location: Great Lakes, USA\n",
      "  title: Developing a Novel Materials Platform for Energy Harvesting Applications\n",
      "  abstract: This project aims to design and synthesize a new class of materials with enhanced energy harvesting capabilities.\n",
      "  keywords: ['materials science', 'energy harvesting', 'nanotechnology']\n",
      "  start_date: 2018-06-01\n",
      "  end_date: 2021-05-31\n",
      "  funding_agency: Department of Energy\n",
      "  location: Cambridge, Massachusetts, USA\n",
      "  title: Investigating the Role of MicroRNAs in Cancer Progression and Metastasis\n",
      "  abstract: This study aims to elucidate the functional significance of microRNAs in cancer progression and metastasis.\n",
      "  keywords: ['cancer biology', 'microRNA', 'epigenetics']\n",
      "  start_date: 2016-01-01\n",
      "  end_date: 2019-12-31\n",
      "  funding_agency: Wellcome Trust\n",
      "  location: New Delhi, India\n",
      "  title: Development of Novel Antimicrobial Materials for Wound Healing\n",
      "  abstract: This study aims to design and synthesize new antimicrobial materials that can accelerate wound healing and prevent infection.\n",
      "  keywords: ['antimicrobial', 'materials science', 'wound healing']\n",
      "  start_date: 2019-06-01\n",
      "  end_date: 2022-05-31\n",
      "  funding_agency: National Institutes of Health (NIH)\n",
      "  location: Cambridge, Massachusetts, USA\n",
      "  title: Investigating the Effects of Climate Change on Coral Reef Ecosystems\n",
      "  abstract: This study aims to investigate the impacts of climate change on coral reef ecosystems and identify potential adaptation strategies.\n",
      "  keywords: ['climate change', 'coral reefs', 'ecosystem resilience']\n",
      "  start_date: 2018-01-01\n",
      "  end_date: 2021-12-31\n",
      "  funding_agency: National Oceanic and Atmospheric Administration (NOAA)\n",
      "  location: Austin, Texas, USA\n",
      "  title: Understanding the Role of Soil Microorganisms in Plant Growth\n",
      "  abstract: This study aims to investigate the effects of soil microorganisms on plant growth and development.\n",
      "  keywords: ['soil microbiology', 'plant biology', 'ecosystem services']\n",
      "  start_date: 2019-09-01\n",
      "  end_date: 2022-08-31\n",
      "  funding_agency: National Science Foundation (NSF)\n",
      "  location: Boston, Massachusetts, USA\n",
      "  title: Investigating the Effects of Noise Pollution on Marine Mammals\n",
      "  abstract: This study aims to investigate the impacts of noise pollution on marine mammal behavior and physiology.\n",
      "  keywords: ['noise pollution', 'marine mammals', 'conservation biology']\n",
      "  start_date: 2018-01-01\n",
      "  end_date: 2021-12-31\n",
      "  funding_agency: National Oceanic and Atmospheric Administration (NOAA)\n",
      "  location: Ann Arbor, Michigan, USA\n",
      "  title: Developing Novel Biomarkers for Cancer Diagnosis and Treatment\n",
      "  abstract: This study aims to identify novel biomarkers for cancer diagnosis and treatment using advanced genomics and bioinformatics approaches.\n",
      "  keywords: ['cancer research', 'biomarkers', 'genomics']\n",
      "  start_date: 2020-03-01\n",
      "  end_date: 2023-02-28\n",
      "  funding_agency: National Institutes of Health (NIH)\n",
      "  location: Palo Alto, California, USA\n",
      "  title: Investigating the Role of Gut Microbiota in Human Health and Disease\n",
      "  abstract: This study aims to elucidate the functional significance of gut microbiota in human health and disease.\n",
      "  keywords: ['gut microbiology', 'human health', 'disease prevention']\n",
      "  start_date: 2018-09-01\n",
      "  end_date: 2022-08-31\n",
      "  funding_agency: National Institutes of Health (NIH)\n",
      "  location: Austin, Texas, USA\n",
      "  title: Examining the Impact of Microplastics on Freshwater Ecosystems\n",
      "  abstract: This study aims to investigate the effects of microplastics on freshwater ecosystems and identify potential mitigation strategies.\n",
      "  keywords: ['microplastics', 'freshwater ecosystems', 'conservation']\n",
      "  start_date: 2020-03-01\n",
      "  end_date: 2023-02-28\n",
      "  funding_agency: Environmental Protection Agency (EPA)\n",
      "  location: Ann Arbor, Michigan, USA\n",
      "  title: Investigating the Role of Nanoparticles in Plant Growth and Development\n",
      "  abstract: This study aims to elucidate the effects of nanoparticles on plant growth and development, with implications for agriculture and environmental sustainability.\n",
      "  keywords: ['nanotechnology', 'plant biology', 'sustainable agriculture']\n",
      "  start_date: 2019-06-01\n",
      "  end_date: 2022-05-31\n",
      "  funding_agency: National Science Foundation (NSF)\n",
      "  location: Stanford, California, USA\n"
     ]
    }
   ],
   "source": [
    "#For metadata handling\n",
    "from json_processing import display_json_data\n",
    "\n",
    "display_json_data('datasets/projects_metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562857c-d86d-42cf-a24b-23a5c3f5cb4e",
   "metadata": {},
   "source": [
    "1.3 - Queries the account and checks the proper setting of details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a39b1-27ab-4e32-b789-b9c369473bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query - GetAccountDetail\n",
    "query = iroha.query('GetAccountDetail',account_id=account['account_id'])\n",
    "IrohaCrypto.sign_query(query, ADMIN_PRIVATE_KEY)\n",
    "response = net.send_query(query)\n",
    "data = response.account_detail_response\n",
    "print(f'Account id = {account}, details = {data.detail}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88167ba4-8e4e-40f7-aadf-0061f3d6b178",
   "metadata": {},
   "source": [
    "1.4 - Queries the account above and extract details to download files and metadatada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36042b70-c047-41d1-86ac-0133851abf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipfs_functions import *\n",
    "from clean_file_name import *\n",
    "\n",
    "# Process the account details response\n",
    "account_details_dict = json.loads(data.detail)  # Convert the string to a JSON object\n",
    "# ic(account_details_dict)\n",
    "\n",
    "# Get the value of the dictionary (the actual file metadata)\n",
    "files_metadata = account_details_dict['admin@test']\n",
    "# ic(files_metadata)\n",
    "\n",
    "\n",
    "for key, value in files_metadata.items():\n",
    "    if 'metadata_CID' not in key:  # Check if this is a file CID\n",
    "        key = '_'.join(key.split('_')[:-1])+\"_CID\"    \n",
    "        ic(key)\n",
    "        file_CID = value\n",
    "        ic(value)\n",
    "        \n",
    "    else:\n",
    "        file_metadata_key = '_'.join(key.split('_')[:-2])  # Extract the actual filename from the key\n",
    "        ic(file_metadata_key)\n",
    "        file_metadata_CID = value  # Get the corresponding metadata CID\n",
    "        ic(file_metadata_CID)\n",
    "        # print(f\"Downloading {file_metadata_CID} metadata...\")\n",
    "        file_metadata_json = download_json_from_ipfs(file_metadata_CID)\n",
    "        # ic(file_metadata_json)\n",
    "        if 'resourceName' in file_metadata_json:  # check if key exists in the dictionary\n",
    "            raw_original_file_name = file_metadata_json['resourceName']\n",
    "            ic(raw_original_file_name)\n",
    "            clean_original_file_name = clean_file_name(raw_original_file_name)  # Remove the 'b' prefix and quotes\n",
    "            ic(clean_original_file_name)\n",
    "\n",
    "            # Create a home directory for the user with the account ID as the username under /download/\n",
    "            user_id = account['account_id']\n",
    "            download_directory = os.path.join(\"download\", user_id)\n",
    "            if not os.path.exists(download_directory):\n",
    "                os.makedirs(download_directory)  # Create the directory if it doesn't exist\n",
    "\n",
    "            file_path = os.path.join(download_directory, clean_original_file_name)\n",
    "            download_file_from_ipfs(file_CID, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad855af-3dbc-4051-9f64-50e2ac51a69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
